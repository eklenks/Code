{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4\n",
    "# Finite-Dimensional Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import append, array, diagonal, tril, triu\n",
    "from numpy.linalg import inv\n",
    "from scipy.linalg import lu\n",
    "#from scipy.linalg import solve\n",
    "from pprint import pprint\n",
    "from numpy import array, zeros, diag, diagflat, dot\n",
    "np.random.seed(123)\n",
    "import warnings\n",
    "%matplotlib notebook\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D     #For 3D plotting\n",
    "from matplotlib import cm   #colormap\n",
    "from sympy import *\n",
    "import sympy as sym\n",
    "init_printing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Newton-Raphson Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A popular function used to test optimization routines is the so-called banana function: \n",
    "\n",
    "$$f = -100*(x_2-x_1^2)^2-(1-x_1)^2$$\n",
    "\n",
    "so-called because its contours resemble bananas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f = lambda x,y:(-100*(y-x**2)**2-(1-x)**2)\n",
    "\n",
    "n = 256\n",
    "x = np.linspace(-0.25, 1.25, n)\n",
    "y = np.linspace(-0.25, 1.25, n)\n",
    "X,Y = np.meshgrid(x, y) \n",
    "\n",
    "plt.figure()\n",
    "x0,y0 = 1,1\n",
    "# use plt.contourf to filling contours\n",
    "# X, Y and value for (X,Y) point\n",
    "plt.contourf(X, Y, f(X, Y), 38, alpha=.75,cmap='bone')# cmap=plt.cm.hot)\n",
    "\n",
    "# use plt.contour to add contour lines\n",
    "C = plt.contour(X, Y, f(X, Y), 38, colors='black', linewidth=.5)\n",
    "\n",
    "plt.clabel(C, inline=True, fontsize=10)\n",
    "# plt.xticks(())\n",
    "# plt.yticks(())\n",
    "\n",
    "# set dot styles\n",
    "plt.scatter([x0, ], [y0, ], s=50, color='b')\n",
    "\n",
    "\n",
    "\n",
    "plt.xlim(-0.25, 1.25)\n",
    "plt.ylim(-0.25, 1.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "\n",
    "ax.contour3D(X, Y, f(X, Y), 50, cmap='binary', alpha=0.3)\n",
    "ax.plot_surface(X, Y, f(X, Y), cmap=cm.coolwarm,alpha=0.7)\n",
    "\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "\n",
    "ax.set_zlim(-100,0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Newton-Raphson method for maximizing an objective function uses successive\n",
    "quadratic approximations to the objective in the hope that the maxima of the approximants\n",
    "will converge to the maximum of the objective. The Newton-Raphson\n",
    "method is intimately related to the Newton method for solving rootfinding problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Indeed, the Newton-Raphson method is identical to applying Newton's method to\n",
    "compute the root of the gradient of the objective function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Taylor series of $f(x)$ about the point  $x=x_0 + \\epsilon$ is given by\n",
    "\n",
    "$$f(x_0 + \\epsilon) = f(x_0)+ f'(x_0) \\epsilon +\\frac{1}{2} f''(x_0) \\epsilon^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f(x) = f(x^{(k)})+ f'(x^{(k)}) (x-x^{(k)}) + \\frac{1}{2}(x-x^{(k)})^T  f''(x^{(k)}) (x-x^{(k)}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solving the first order condition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f'(x^{(k)})+ f''(x^{(k)}) (x-x^{(k)}) = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "yields the iteration rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "$$x^{(k+1)} \\leftarrow x^{(k)} -  [f''(x^{(k)})]^{-1} f'(x^{(k)}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In theory, the Newton-Raphson method converges if $f$ is twice continuously difierentiable\n",
    "and if the initial value of x supplied by the analyst is sufficiently close to a\n",
    "local maximum of $f$ at which the **Hessian $f''$** is negative definite. There is, however,\n",
    "no generally practical formula for determining what sufficiently close is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Newton-Raphson method can be robust to the starting\n",
    "value if $f$ is well behaved, for example, if f is **globally concave**. The Newton-Raphson\n",
    "method, however, can be very sensitive to starting value if the function is not globally\n",
    "concave. Also, in practice, the **Hessian $f''$**  must be well-conditioned at the optimum,\n",
    "otherwise rounding errors in the vicinity of the optimum can make it difficult to\n",
    "compute a precise approximate solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Newton-Raphson algorithm has numerous drawbacks. \n",
    "\n",
    "First, the algorithm\n",
    "requires computation of both the first and second derivatives of the objective function.\n",
    "\n",
    "\n",
    "Second, the Newton-Raphson algorithm offers no **guarantee** that the objective function\n",
    "value may be increased in the direction of the Newton step. Such a guarantee is\n",
    "available only if the Hessian **Hessian $f''(x^k)$** is **negative definite**; otherwise, one may actually\n",
    "move towards a saddle point of f (if the Hessian is indefinite) or even a minimum (if\n",
    "Hessian is **positive definite**).\n",
    "\n",
    "For this reason, the Newton-Raphson method is rarely\n",
    "used in practice, and then only if the objective function is **globally concave**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Quasi-Newton Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Quasi-Newton methods employ a similar strategy to the Newton-Raphson method,\n",
    "but **replace the Hessian of the objective function (or its inverse) with a negative\n",
    "definite approximation, guaranteeing that function value can be increased in the direction\n",
    "of the Newton step**. \n",
    "\n",
    "The most efficient quasi-Newton algorithms employ an\n",
    "approximation to the inverse Hessian, rather than the Hessian itself, in order to avoid\n",
    "performing a linear solve, and employ updating rules that do **not require second\n",
    "derivative information** to ease the burden of implementation and the cost of computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In analogy with the Newton-Raphson method, quasi-Newton methods use a search\n",
    "direction of the form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$d^{(k)} = -B^{(k)} f'(x^{(k)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $B^{(k)}$ is an approximation to the **inverse Hessian** of f at the kth iterate $x^{(k)}$.\n",
    "The vector $d^{(k)}$ is called the **Newton or quasi-Newton step**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The more robust quasi-Newton methods do not necessarily take the full Newton\n",
    "step, but rather shorten it or lengthen it in order to obtain improvement in the\n",
    "objective function. This is accomplished by performing a line-search in which one\n",
    "seeks a **step length $s > 0$** that maximizes or nearly maximizes $f (x^{(k)} + sd^{(k)})$. Given\n",
    "the computed step length $s^{(k)}$, one updates the iterate as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$x^{(k+1)}=  x^{(k)} + s^{(k)}  d^{(k)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quasi-Newton methods differ in how the inverse Hessian approximation Bk is constructed\n",
    "and updated. The simplest quasi-Newton method sets\n",
    "\n",
    "$$B^{(k)} = - I $$\n",
    "\n",
    "where I is  the identity matrix. This leads to a Newton step that is identical to the gradient of\n",
    "the objective function at the current iterate:\n",
    "\n",
    "\n",
    "$$d^{(k)} = f'(x^{(k)})$$\n",
    "\n",
    "The choice of gradient as a step direction is intuitively appealing because the gradient\n",
    "always points in the direction which, to a first order, promises the greatest increase in\n",
    "$f$. For this reason, this quasi-Newton method is called the method of *steepest ascent.*\n",
    "\n",
    "\n",
    "The steepest ascent method is simple to implement, but is numerically *less efficient*\n",
    "in practice than competing quasi-Newton methods that *incorporate* information regarding\n",
    "the **curvature of the objective function**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **most widely-used** quasi-Newton methods that employ **curvature information**\n",
    "produce a sequence of inverse Hessian estimates that satisfy two conditions. \n",
    "\n",
    "**First,**\n",
    "given that\n",
    "\n",
    "\n",
    "$$d^{(k)} \\approx f''^{-1}(x^{(k)})( f'(x^{(k)}+ d^{(k)} ) - f'(x^{(k)})  )$$\n",
    "\n",
    "the **inverse Hessian estimate** $B^{(k)}$ is required to satisfy the so-called **quasi-Newton condition:**\n",
    "\n",
    "$$d^{(k)} = B^{(k)}( f'(x^{(k)}+ d^{(k)} ) - f'(x^{(k)})  )$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Second,** the inverse Hessian estimate $B^{(k)}$  is required to be both **symmetric and\n",
    "negative-definite**, as must be true of the inverse Hessian at a local maximum. The\n",
    "negative definiteness of the Hessian estimate assures that the objective function value\n",
    "can be inreased in the **direction of the Newton step**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two methods that satisfy the quasi-Newton and negative definiteness conditions\n",
    "are the **Davidson-Fletcher-Powell (DFP)** and **Broyden-Fletcher-Goldfarb-Shano (BFGS)**\n",
    "updating methods. The **DFP** method uses the updating scheme\n",
    "\n",
    "\n",
    "$$B \\leftarrow B + \\frac{d d^T}{d^T u}  - \\frac{B u u^T B}{u^T B u} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where\n",
    "\n",
    "$$d = x^{(k+1)} - x^{(k)}$$\n",
    "\n",
    "and \n",
    "\n",
    "$$u = f'(x^{(k+1)}) - f'(x^{(k)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **BFGS** method uses the update scheme\n",
    "\n",
    "\n",
    "$$B \\leftarrow B + \\frac{1}{d^T u}( w d^T + d w^T  - \\frac{w^T u}{d^T u}) d d^T $$\n",
    "\n",
    "where \n",
    "\n",
    "$$w = d - B u$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BFGS algorithm is generally considered superior to DFP, although there\n",
    "are problems for which DFP outperforms BFGS. However, except for the updating\n",
    "formulae, the two methods are identical, so it is easy to implement both and give\n",
    "users the choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list\n",
    "step_methods = ['none','bhhh','bt','golden']                         \n",
    "search_methods = ['Steepest','DFP','BFGS']                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary\n",
    "# step_methods = {'none': _step_none,\n",
    "#                      'bhhh': _step_bhhh,\n",
    "#                      'bt': _step_bt,\n",
    "#                      'golden': _step_golden\n",
    "#                      }\n",
    "# search_methods = {'steepest': _search_steepest,\n",
    "#                        'dfp': _search_dfp\n",
    "#                        'bfgs': _search_bfgs,\n",
    "#                        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _search_bfgs(f, ff=None, u=None, d=None):\n",
    "#         ud = np.inner(u, d)\n",
    "#         w = d - B.dot(u)\n",
    "#         wd = np.outer(w, d)\n",
    "#         return  B+ ((wd + wd.T) - (np.inner(u, w) * np.outer(d, d)) / ud) / ud\n",
    "#         # self.reset = False\n",
    "\n",
    "# def _search_dfp(self, ff=None, u=None, d=None):\n",
    "#         ud = np.inner(u, d)\n",
    "#         v = B.dot(u)\n",
    "#         return B+ np.outer(d, d) / ud - np.outer(v, v) / np.inner(u, v)\n",
    "#         #self.reset = False\n",
    "\n",
    "# def _search_steepest(self, ff, u=None, d=None):\n",
    "#     return   -np.identity(k) / np.maximum(abs(fx0), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Find the best step size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function optstep is not covered in the textbook.\n",
    "# It is used to implement the qnewton routine.\n",
    "errcode = False\n",
    "def optstep(stepmeth,func, x0, fx0, g0, d, maxstep = 1000):\n",
    "    \"\"\"find the best step size\"\"\"\n",
    "    # take multiple output of function\n",
    "    A = func(x)\n",
    "    k = x0.shape[0] # number of variables\n",
    "    _is_there_jacobian = (type(A) is tuple) and (len(A) == 2)\n",
    "\n",
    "    if _is_there_jacobian:\n",
    "        #print('Jacobian was provided by user!')\n",
    "        f = lambda z:  func(z)[0]\n",
    "        \n",
    "    # several step search method\n",
    "    def _step_none(f, x0, fx0, d,maxstep):\n",
    "        fx = f(x0 + d)\n",
    "        if fx < fx0:\n",
    "            s = 1\n",
    "            errcode = False\n",
    "            return s, f\n",
    "        else:\n",
    "            return _step_golden(f, x0, fx0, d,maxstep)\n",
    "\n",
    "    def _step_bhhh(f, x0, fx0, g0, d,maxstep):\n",
    "        # Intializations\n",
    "        delta = 0.0001\n",
    "        dg = -np.inner(g0, d)  # directional derivative\n",
    "        tol1 = dg * delta\n",
    "        tol0 = dg * (1 - delta)\n",
    "        s, ds = 1, 1\n",
    "        errcode = False\n",
    "\n",
    "        # Bracket the cone\n",
    "        for it in range(maxstep):\n",
    "            x = x0 + s * d\n",
    "            fs = f(x)\n",
    "            temp = (fx0 - fs) / s\n",
    "            if temp < tol0:\n",
    "                ds *= 2\n",
    "                s += ds\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        if (tol0 <= temp) and (temp <=tol1):\n",
    "            return s, fs\n",
    "\n",
    "        ds /= 2\n",
    "        s -= ds\n",
    "        it0 = it + 1\n",
    "\n",
    "        # Then use bisection to get inside it\n",
    "        for it in range(it0, maxstep):\n",
    "            ds /= 2\n",
    "            x = x0 + s * d\n",
    "            fs =  f(x)\n",
    "            temp = (fx0 - fs) / s\n",
    "            if temp > tol1:\n",
    "                s -= ds\n",
    "            elif temp < tol0:\n",
    "                s += ds\n",
    "            else:\n",
    "                return s, fs\n",
    "\n",
    "        # If it has not returned yet, call _step_golden!\n",
    "        return _step_golden(f, x0, fx0, d, maxstep)\n",
    "\n",
    "    def _step_bt(f, x0, fx0, g0, d, maxstep):\n",
    "        delta = 1e-4 # Defines cone of convergence; must be on (0,1/2)\n",
    "        ub = 0.5     # Upper bound on acceptable reduction in s.\n",
    "        lb = 0.1     # Lower bound on acceptable reduction in s.\n",
    "        errcode = 0\n",
    "        dg = -np.inner(d, g0)  # directional derivative\n",
    "        tol1 = delta * dg\n",
    "        tol0 = (1 - delta) * dg\n",
    "\n",
    "        # full step\n",
    "        s = 1\n",
    "        fs = f(x0+d)\n",
    "        if (fx0 - fs)   <= tol1:\n",
    "            return s, fs\n",
    "\n",
    "        # quadratic approximation\n",
    "        s2, fs2 = s, fs\n",
    "        s = -0.5 * dg / (-fs + fx0 - dg)\n",
    "        s = max(s, lb)\n",
    "        fs = f(x0 + s * d)\n",
    "        temp = (-fs + fx0) / s\n",
    "        if (tol0 <= temp) and (temp <= tol1):\n",
    "            return s, fs\n",
    "\n",
    "        # cubic approximation\n",
    "        for it in range(3, maxstep):\n",
    "            temp = (s - s2) * np.array([s * s, s2 * s2])\n",
    "            temp = np.array([- fs + fx0 - dg * s, -fs2 + fx0 - dg * s2]) / temp\n",
    "            a = temp[0] - temp[1]\n",
    "            b = s * temp[1] - s2 * temp[0]\n",
    "            s2 = s\n",
    "            fs2 = fs\n",
    "            if np.all(a == 0):  # quadratic fits exactly\n",
    "                s = -0.5 * dg / b\n",
    "            else:\n",
    "                disc = b * b - 3 * a * dg\n",
    "                if np.all(disc < 0):\n",
    "                    errcode = 2\n",
    "                    return s, fs  # complex root\n",
    "                s = (np.sqrt(disc) - b) / (3 * a)\n",
    "\n",
    "            s = np.maximum(np.minimum(s, ub * s2), lb * s2)  # ensures acceptable step size; cp(f, lb, up)\n",
    "            fs = f(x0 + s * d)\n",
    "            temp = (-fs + fx0) / s\n",
    "            if np.all(tol0 <= temp) and np.all(temp <= tol1):\n",
    "                return s, fs\n",
    "\n",
    "        # If it has not returned yet, call _step_golden instead\n",
    "        return _step_golden(f, x0, fx0, d,maxstep)\n",
    "\n",
    "    def _step_golden(f, x0, fx0, d,maxstep):\n",
    "        alpha1 = (3 - np.sqrt(5)) / 2\n",
    "        alpha2 = (np.sqrt(5) - 1) / 2\n",
    "        tol = 1.e-4\n",
    "        tol *= alpha1*alpha2\n",
    "        s = 1\n",
    "        errcode = True\n",
    "        niter = 0\n",
    "        s0 = 0\n",
    "        it = 0\n",
    "\n",
    "        # Find a bracketing interval\n",
    "        fs = f(x0 + d)\n",
    "        if fx0 >= fs:\n",
    "            lenght = alpha1\n",
    "        else:\n",
    "            for it in range(maxstep):\n",
    "                s *= 2\n",
    "                fl = fs\n",
    "                fs = f(x0 + s*d)\n",
    "                if fs <=fl:\n",
    "                    lenght = alpha1 * (s - s0)\n",
    "                    break\n",
    "                else:\n",
    "                    s0 /= 2\n",
    "\n",
    "            if (it + 1) >= maxstep:\n",
    "                s /= 2\n",
    "                fs = fl\n",
    "                return s, fs\n",
    "\n",
    "        xl = x0 + (s + lenght) * d\n",
    "        xs = x0 + (s - lenght) * d\n",
    "\n",
    "        s -= lenght\n",
    "        lenght *= alpha2  # lenght now measures relative distance between xl and xs\n",
    "\n",
    "        fs = f(xs)\n",
    "        fl = f(xl)\n",
    "\n",
    "        # Golden search to find minimum\n",
    "        while it < maxstep:\n",
    "            it += 1\n",
    "            if fs < fl:\n",
    "                s -= lenght\n",
    "                lenght *= alpha2\n",
    "                xs = xl\n",
    "                xl -= lenght * d\n",
    "                fs = fl\n",
    "                fl = f(xl)\n",
    "            else:\n",
    "                lenght *= alpha2\n",
    "                s += lenght\n",
    "                xl = xs\n",
    "                xs += lenght * d\n",
    "                fl = fs\n",
    "                fs = f(xs)\n",
    "\n",
    "            if lenght < tol:\n",
    "                errcode = False\n",
    "                break\n",
    "        if fl > fs:\n",
    "            fs = fl\n",
    "            s -= lenght\n",
    "        return s, fs\n",
    "    \n",
    "    # return resulted s and fx\n",
    "    if stepmeth == None:\n",
    "        return _step_none(f, x0, fx0, d,maxstep)\n",
    "    elif stepmeth == \"bhhh\":\n",
    "        return _step_bhhh(f, x0, fx0, g0, d,maxstep)\n",
    "    elif stepmeth == \"bt\":\n",
    "        return _step_bt(f, x0, fx0, g0, d,maxstep)\n",
    "    elif stepmeth == \"golden\":\n",
    "        return _step_golden(f, x0, fx0, d,maxstep)\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Banana function\n",
    "def f(x):\n",
    "    y = (-100*(x[1]-x[0]**2)**2-(1-x[0])**2)\n",
    "    dy = np.array([2*(1-x[0])+400*(x[1]-x[0]**2)*x[0],  -200*(x[1]-x[0]**2)])\n",
    "    return y,dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script assumes that the user\n",
    "has written a Python routine $f$ that evaluates the function at an arbitrary point and\n",
    "that the user has specified a starting point $x$, an initial guess for the inverse Hessian\n",
    "$A$, a convergence tolerance tol, and a limit on the number of iterations maxit. The\n",
    "script uses an auxiliary algorithm optstep to determine the step length (discussed\n",
    "in the next section). The algorithm also offers the user a choice on how to select the\n",
    "search direction, searchmeth (1-steepest ascent, 2-DFP, 3-BFGS).\n",
    "\n",
    "https://docs.scipy.org/doc/scipy/reference/tutorial/optimize.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#         if self.x0 is None or self.x0[0] is None:\n",
    "#             raise ValueError('Initial value is required to solve a OP, none provided!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this cell we select the search method and the step method\n",
    "\n",
    "x_list = list()#  sequence of solutions of x for plotting\n",
    "\n",
    "x0 = np.array([1.,0.]) # initial value for x\n",
    "\n",
    "maxit, maxstep, tol,eps0, eps1,all_x  = 10000, 10000, 1/10000,1.0,1.e-12 ,False # keyword arguments\n",
    "\n",
    "x_list = [x0] # first x\n",
    "searchmeth =2 #  pick a search method.\n",
    "stepmeth = \"bt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x0 # initialize\n",
    "k = x.shape[0] # number of variables\n",
    "eps = np.spacing(1) # epsilon\n",
    "\n",
    "A = f(x) # tuble of multiple outputs from object function\n",
    "\n",
    "_is_there_jacobian = (type(A) is tuple) and (len(A) == 2)\n",
    "\n",
    "# get first fx and g. object value and gradient/hessian value. \n",
    "if _is_there_jacobian:\n",
    "    print('Jacobian was provided by user!')\n",
    "    fx0,g0 = f(x)\n",
    "else:    \n",
    "    print('Jacobian was not provided by user!')\n",
    "    fx0 = f(x)\n",
    "    try:\n",
    "        g0 = jacobian(f,x) # customized jacobian function\n",
    "    except NameError:\n",
    "        print(\"Jacobian function Not in scope!\\n Using identity matrix as Jacobian matrix\")\n",
    "        g0 = np.identity(k)\n",
    "    else:\n",
    "        print(\"Jacobian function In scope!\")\n",
    "\n",
    "B = None # inversed Hessian matrix        \n",
    "        \n",
    "if B is None:\n",
    "    B =  -np.identity(k) / np.maximum(abs(fx0), 1)   # using identity matrix as Hessian\n",
    "    print(\"Hessian is not provided and reset as normalized identity matrix! so steepest ascent\") # steepest ascent\n",
    "    \n",
    "d = -np.dot(B, g0)  # search direction    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell shows one update based on a specific method\n",
    "s, fx = optstep(\"bt\", f, x, fx0, g0, d, maxstep)\n",
    "\n",
    "s,fx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell shows one update based on a different method\n",
    "s, fx = optstep(\"golden\", f, x0, fx0, g0, d, maxstep)\n",
    "s,fx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell solves the maximization problem, relying on the functions defined above\n",
    "\n",
    "if np.linalg.norm(g0) < eps: # similar to np.all(g0<eps)\n",
    "    #break #return x\n",
    "    print(\"g0 is less than eps\")\n",
    "if np.all(g0 < eps): # check conditions\n",
    "    #break #return x\n",
    "    print(\"g0 is less than eps\")\n",
    "print(\"Solving optimization by using {} search method and {} step method\".format(search_methods[searchmeth-1].capitalize(), stepmeth)) \n",
    "\n",
    "print(\"Start iteration......\")\n",
    "\n",
    "\n",
    "for it in range(maxit):\n",
    "    \n",
    "    \n",
    "    d = -np.dot(B, g0)  # search direction\n",
    "    \n",
    "    if (np.inner(d, g0) / (np.inner(d, d))) < eps1:  # must go uphill\n",
    "        B =  -np.identity(k) / np.maximum(abs(fx0), 1) # otherwise use\n",
    "        d = g0 / np.maximum(np.abs(fx0), 1)  # steepest ascent\n",
    "\n",
    "    s, fx = optstep(\"bt\", f, x, fx0, g0, d, maxstep)\n",
    "    \n",
    "    if fx <= fx0:\n",
    "        \n",
    "        warnings.warn('Iterations stuck in qnewton')\n",
    "        # break #x # return x\n",
    "        # reset Hessian and d.\n",
    "        B =  -np.identity(k) / np.maximum(abs(fx0), 1) # otherwise use\n",
    "        d = g0.T / np.maximum(abs(fx0), 1)  # steepest ascent\n",
    "        s, fx = optstep(\"bt\" , f, x, fx0, g0, d, maxstep)\n",
    "        if errcode:\n",
    "            warnings.warn('Cannot find suitable step in qnewton')\n",
    "            # return x\n",
    "            # reset to 1 and fx0\n",
    "            s, fx =  1, fx0\n",
    "    \n",
    "    \n",
    "    d *= s\n",
    "    x = x + d\n",
    "    \n",
    "    x_list.append(x.copy())\n",
    "\n",
    "    if np.any(np.isnan(x) | np.isinf(x)):\n",
    "        raise ValueError('NaNs or Infs encountered')\n",
    "    \n",
    "    # update fx and g\n",
    "    if _is_there_jacobian:\n",
    "        #print('Jacobian was provided by user!')\n",
    "        fx,g = f(x)\n",
    "    else:    \n",
    "        print('Jacobian was not provided by user!')\n",
    "        fx = f(x)\n",
    "        try:\n",
    "            g = jacobian(f,x)\n",
    "        except NameError:\n",
    "            print(\"Jacobian function Not in scope!\\n Using identity matrix as Jacobian matrix\")\n",
    "            g = np.identity(k)\n",
    "        else:\n",
    "            print(\"Jacobian function In scope!\")\n",
    "    \n",
    "\n",
    "    # Test convergence using Marquardt's criteria and gradient test\n",
    "    if ((fx - fx0) / (abs(fx) + eps0) < tol and\n",
    "            np.all(np.abs(d) / (np.abs(x) + eps0) < tol)) or\\\n",
    "            np.all(np.abs(g) < eps):\n",
    "            print(\"Meet the tol. x: \", x)\n",
    "            break\n",
    "#         #return x\n",
    "\n",
    "#     if np.all( np.abs(d)/(np.abs(x) + eps0)< tol) or np.all(np.abs(g) < eps):\n",
    "#         print(\"Meet the tol. x: \", x)\n",
    "#         break\n",
    "        \n",
    "\n",
    "    # Update inverse Hessian\n",
    "    u = g - g0  # change in Jacobian\n",
    "    ud = np.inner(u, d)\n",
    "    \n",
    "    \n",
    "    #print(\"Please specify one search method: 1:steepest ascen;2: DFP;3:BFGS\")\n",
    "    if np.all(np.abs(ud) < eps):\n",
    "        B =  -np.identity(k) / np.maximum(abs(fx0), 1) # otherwise use\n",
    "    else:\n",
    "        if searchmeth == 1 and np.abs(ud) < eps: # steepest ascent\n",
    "            B =  -np.identity(k) / np.maximum(abs(fx), 1)\n",
    "        elif searchmeth == 2: # DFP\n",
    "            v = B.dot(u)\n",
    "            B += np.outer(d, d) / ud - np.outer(v, v) / np.inner(u, v) \n",
    "        elif searchmeth == 3: # BFGS\n",
    "            w = d - B.dot(u)\n",
    "            wd = np.outer(w, d)\n",
    "            B += ((wd + wd.T) - (np.inner(u, w) * np.outer(d, d)) / ud) / ud\n",
    "#         else:\n",
    "#             print(\"Please specify one search method: 1:steepest ascen;2: DFP;3:BFGS\")\n",
    "\n",
    "    # Update iteration\n",
    "    fx0 = fx\n",
    "    g0 = g\n",
    "    print(\"Completed {}th iteration...\".format(it))   \n",
    "    \n",
    "#print(\"x list: \" +  for str(x) in x_list)    \n",
    "if it > maxit:\n",
    "    warnings.warn('Maximum iterations exceeded in qnewton')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell reports the whole sequence of guesses/iterations\n",
    "x_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.array([1.,0.]) # initial value for x\n",
    "\n",
    "B = None # inverse Hessian matrix        \n",
    "        \n",
    "if B is None:\n",
    "    B =  -np.identity(k) / np.maximum(abs(fx0), 1)   # using identity matrix as Hessian\n",
    "    print(\"Hessian is not provided and reset as normalized identity matrix! so steepest ascent\") # steepest ascent\n",
    "\n",
    "\n",
    "f,x0,B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This part of the code is a general implementation of quasi-Netwon methods\n",
    "\n",
    "def myqnewton(f, x0, B=None, searchmeth = 3, stepmeth = \"bt\", maxit = 10000, maxstep = 10000, tol = 1/100000,\\\n",
    "              eps = np.spacing(1), eps0 =1.0, eps1 = 1.e-12, all_x = False):\n",
    "    '''\n",
    "    maxit, maxstep, tol,eps0, eps1  = 10000, 10000, 1/10000,1.0,1.e-12\n",
    "    f: object function and jacobian\n",
    "    x0: initial value\n",
    "    all_x: if we collect x value for plotting\n",
    "    '''\n",
    "    x = x0\n",
    "    if all_x:\n",
    "        x_list = [x0]\n",
    "        \n",
    "    k = x.shape[0] # number of variables\n",
    "    A = f(x)\n",
    "    _is_there_jacobian = (type(A) is tuple) and (len(A) == 2)\n",
    "\n",
    "    if _is_there_jacobian:\n",
    "        print('Jacobian was provided by user!')\n",
    "        fx0,g0 = f(x)\n",
    "    else:    \n",
    "        print('Jacobian was not provided by user!')\n",
    "        fx0 = f(x)\n",
    "        try:\n",
    "            g0 = jacobian(f,x)\n",
    "        except NameError:\n",
    "            print(\"Jacobian function Not in scope!\\n Using identity matrix as Jacobian matrix\")\n",
    "            g0 = np.identity(k)\n",
    "        else:\n",
    "            print(\"Jacobian function In scope!\")    \n",
    "        \n",
    "    if np.all(np.abs(g0) < eps): # similar to np.all(g0<eps)\n",
    "        print(\"abs(g0)< eps...\")\n",
    "        return x\n",
    "    \n",
    "    print(\"Solving optimization by using {} search method and {} step method\".format(search_methods[searchmeth-1].capitalize(), stepmeth)) \n",
    "\n",
    "    print(\"Start iteration......\")\n",
    "    \n",
    "    #B = None # inversed Hessian matrix        \n",
    "        \n",
    "    if B is None:\n",
    "        B =  -np.identity(k) / np.maximum(abs(fx0), 1)   # using identity matrix as Hessian\n",
    "        print(\"Hessian is not provided and reset as normalized identity matrix! so steepest ascent\") # steepest ascent\n",
    "\n",
    "    for it in range(maxit):\n",
    "\n",
    "        d = -np.dot(B, g0)  # search direction, initial d\n",
    "        \n",
    "        # https://github.com/randall-romero/CompEcon-python/blob/master/compecon/optimize.py\n",
    "        if (np.inner(d, g0) / (np.inner(d, d))) < eps1:  # must go uphill\n",
    "            B =  -np.identity(k) / np.maximum(abs(fx0), 1) # otherwise use\n",
    "            d = g0 / np.maximum(np.abs(fx0), 1)  # steepest ascent\n",
    "        # optimize search step length\n",
    "        s, fx = optstep(stepmeth ,f, x, fx0, g0, d, maxstep)\n",
    "\n",
    "        if fx <= fx0:\n",
    "\n",
    "            warnings.warn('Iterations stuck in qnewton')\n",
    "            #return x\n",
    "            # reset Hessian and d.\n",
    "            B =  -np.identity(k) / np.maximum(abs(fx0), 1) # otherwise use\n",
    "            d = g0.T / np.maximum(abs(fx0), 1)  # steepest ascent\n",
    "            s, fx = optstep(\"bt\" ,f, x, fx0, g0, d, maxstep)\n",
    "            if errcode:\n",
    "                warnings.warn('Cannot find suitable step in qnewton')\n",
    "                # return x\n",
    "                # reset to 1 and fx0\n",
    "                s, fx =  1, fx0\n",
    "\n",
    "        # update d and x\n",
    "        d *= s\n",
    "        x = x + d\n",
    "        # keep record of x sequence in list\n",
    "        if all_x:\n",
    "            x_list.append(x.copy())\n",
    "\n",
    "        if np.any(np.isnan(x) | np.isinf(x)):\n",
    "            raise ValueError('NaNs or Infs encountered')\n",
    "\n",
    "        #  update fx and g again\n",
    "        if _is_there_jacobian:\n",
    "            #print('Jacobian was provided by user!')\n",
    "            fx,g = f(x)\n",
    "        else:    \n",
    "            print('Jacobian was not provided by user!')\n",
    "            fx = f(x)\n",
    "            try:\n",
    "                g = jacobian(f,x)\n",
    "            except NameError:\n",
    "                print(\"Jacobian function Not in scope!\\n Using identity matrix as Jacobian matrix\")\n",
    "                g = np.identity(k)\n",
    "            else:\n",
    "                print(\"Jacobian function In scope!\")\n",
    "\n",
    "        # Test convergence using Marquardt's criteria and gradient test\n",
    "        if ((fx - fx0) / (abs(fx) + eps0) < tol and\n",
    "                np.all(np.abs(d) / (np.abs(x) + eps0) < tol)) or\\\n",
    "                np.all(np.abs(g) < eps):\n",
    "                print(\"Meet the tol. x: \", x)\n",
    "                #break\n",
    "                if all_x:\n",
    "                    return x, x_list\n",
    "                else:\n",
    "                    return x\n",
    "\n",
    "        # Update inverse Hessian\n",
    "        u = g - g0  # change in Jacobian\n",
    "        ud = np.inner(u, d)\n",
    "\n",
    "        # pick a search method\n",
    "        #print(\"Please specify one search method: 1:steepest ascen;2: DFP;3:BFGS\")\n",
    "        if np.all(np.abs(ud) < eps):\n",
    "            B =  -np.identity(k) / np.maximum(abs(fx0), 1) # otherwise use\n",
    "        else:\n",
    "            if searchmeth == 1 and np.abs(ud) < eps: # steepest ascent\n",
    "                B =  -np.identity(k) / np.maximum(abs(fx), 1)\n",
    "            elif searchmeth == 2: # DFP\n",
    "                v = B.dot(u)\n",
    "                B += np.outer(d, d) / ud - np.outer(v, v) / np.inner(u, v) \n",
    "            elif searchmeth == 3: # BFGS\n",
    "                w = d - B.dot(u)\n",
    "                wd = np.outer(w, d)\n",
    "                B += ((wd + wd.T) - (np.inner(u, w) * np.outer(d, d)) / ud) / ud\n",
    "\n",
    "        # Update iteration\n",
    "        fx0 = fx\n",
    "        g0 = g\n",
    "        print(\"Completed {}th iteration...\".format(it))   \n",
    "\n",
    "    # end of iteration if exceed the maxit\n",
    "    if it >= maxit:\n",
    "        warnings.warn('Maximum iterations exceeded in qnewton')\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here and in the next three cells we compare the perfomance of different methods using a benchmark case.\n",
    "x0 = np.array([1.,0.]) # initial value for x\n",
    "B = None # inverse Hessian matrix   \n",
    "myqnewton(f, x0, B, searchmeth = 3,stepmeth = \"bt\" ,maxit = 10000, maxstep = 10000,tol = 1/100000,\\\n",
    "              eps = np.spacing(1),eps0 =1.0, eps1 = 1.e-12, all_x = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.array([1.,0.]) # initial value for x\n",
    "B = None # inverse Hessian matrix    \n",
    "myqnewton(f, x0, B, searchmeth = 2,stepmeth = \"bt\" ,maxit = 10000, maxstep = 10000,tol = 1/100000,\\\n",
    "              eps = np.spacing(1),eps0 =1.0, eps1 = 1.e-12, all_x = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.array([1.,0.]) # initial value for x\n",
    "myqnewton(f, x0,B = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.array([1.,0.]) # initial value for x\n",
    "B = None # inverse Hessian matrix   \n",
    "myqnewton(f, x0, B, searchmeth = 2, stepmeth = \"bt\", maxit = 10000, maxstep = 10000, tol = 1/100000,\\\n",
    "              eps = np.spacing(1), eps0 =1.0, eps1 = 1.e-12, all_x = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.array([1.,0.]) # initial value for x\n",
    "B = None # inverse Hessian matrix   \n",
    "myqnewton(f, x0, B, searchmeth =1, stepmeth = \"bt\", maxit = 10000, maxstep = 10000, tol = 1/100000,\\\n",
    "              eps = np.spacing(1), eps0 =1.0, eps1 = 1.e-12, all_x = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4.4 Line Search Methods\n",
    "Just as was the case with rootfinding problems, it is not always best to take a full\n",
    "Newton step. In fact, it may be better to either stop short or move past the Newton\n",
    "step. If we view the Newton step as defining a *search direction*, performing a onedimensional\n",
    "search in that direction will generally produce improved results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "http://reference.wolfram.com/language/tutorial/UnconstrainedOptimizationLineSearchMethods.html\n",
    "\n",
    "https://en.wikipedia.org/wiki/Line_search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A number of diffierent line\n",
    "search methods are used in practice, including the golden search method. \n",
    "\n",
    "The **golden search** algorithm is very reliable, but computationally inefficient. Two alternative\n",
    "schemes are typically used in practice to perform line searches. \n",
    "\n",
    "The first, known as\n",
    "the **Armijo search**, is similar to the backstepping algorithm used in rootfinding and complementarity problems. The idea is to find the minimum power $j$ such that:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/smwade/ACME-2/blob/master/line_search/solutions.py\n",
    "\n",
    "def backtracking(f, slope, x, p, a=1, rho=.9, c=10e-4):\n",
    "    \"\"\"Perform a backtracking line search to satisfy the Armijo Conditions.\n",
    "    Parameters:\n",
    "        f (function): the twice-differentiable objective function.\n",
    "        slope (float): The value of grad(f)^T p.\n",
    "        x (ndarray of shape (n,)): The current iterate.\n",
    "        p (ndarray of shape (n,)): The current search direction.\n",
    "        a (float): The intial step length. (set to 1 in Newton and\n",
    "            quasi-Newton methods)\n",
    "        rho (float): A number in (0,1).\n",
    "        c (float): A number in (0,1).\n",
    "    \n",
    "    Returns:\n",
    "        (float) The computed step size satisfying the Armijo condition.\n",
    "    \"\"\"\n",
    "    while f(x + a*p) > f(x) + c * a * slope:\n",
    "        a = float(rho * a)\n",
    "    return a\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Another widely-used approach, known as **Goldstein search**, is to find any value of $s$ that satisfies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple strategy for locating an acceptable point is to first find a point in or\n",
    "above the cone using step doubling (doubling the value of s at each iteration). If a\n",
    "point above the cone is found first, we have a bracket within which points in the cone\n",
    "must lie. We can then narrow the bracket using the golden search method. We call this the bhhhstep approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another approach, stepbt, checks to see if $s = 1$ is in the cone and, if so, maximizes\n",
    "a quadratic approximation to the objective function in the Newton direction\n",
    "constructed from knowledge of $f(x)$, $f(x)d$ and $f(x + d)$. If the computed step $s$ is\n",
    "acceptable, it is taken. Otherwise, the algorithm iterates until an acceptable step is\n",
    "found using a cubic approximation to the objective function in the Newton direction\n",
    "constructed from knowledge of $f(x)$, $f(x)d$, $f(x + s(j+1)d)$ and $f(x + s(j)d)$. stepbt\n",
    "is fast and generally gives good results. It is recommended as the default lines search\n",
    "procedure for general maximization algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##  4.5 Special Cases\n",
    "Two special cases arise often enough in economic practice (especially in econometrics)\n",
    "to warrant additional discussion. Nonlinear least squares and the maximum likelihood\n",
    "problems have objective functions with special structures that give rise to their\n",
    "own special quasi-Newton methods. The special methods differ from other Newton\n",
    "and quasi-Newton methods only in the choice of the matrix used to approximate the\n",
    "Hessian. Because these problems generally arise in the context of statistical applications,\n",
    "we alter our notation to conform with the conventions for those applications.\n",
    "The optimization takes place with respect to a $k$-dimensional parameter vector $\\theta$ and\n",
    "$n$ will refer to the number of observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4.6 Scipy Optimization\n",
    "\n",
    "Often we need to find the maximum or minimum of a particular function $f(x)$ where $f$ is a scalar function but $x$ could be a vector. Typical applications are the minimisation of entities such as cost, risk and error, or the maximisation of productivity, efficiency and profit. Optimisation routines typically provide a method to minimise a given function: if we need to maximise $f(x)$ we create a new function $g(x)$ that reverses the sign of $f$, i.e. $g(x)= − f(x)$ and we minimise $g(x)$.\n",
    "\n",
    "Below, we provide an example showing (i) the definition of the test function and (ii) the call of the `scipy.optimize.fmin` function which takes as argument a function $f$ to minimise and an initial value $x_0$ from which to start the search for the minimum, and which returns the value of $x$ for which $f(x)$ is (locally) minimised. Typically, the search for the minimum is a local search, i.e. the algorithm follows the local gradient. We repeat the search for the minimum for two values ($x_0 = 1.0$ and $x_0 = 2.0$, respectively) to demonstrate that depending on the starting value we may find different minimar of the function f.\n",
    "\n",
    "The majority of the commands (after the two calls to `fmin`) creates the plot of the function, the start points for the searches and the minima obtained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from numpy import arange, cos, exp\n",
    "from scipy.optimize import fmin\n",
    "\n",
    "def f(x):\n",
    "    return cos(x) - 3 * exp( -(x - 0.2) ** 2)\n",
    "\n",
    "# find minima of f(x),\n",
    "# starting from 1.0 and 2.0 respectively\n",
    "minimum1 = fmin(f, 1.0)\n",
    "print(\"Start search at x=1., minimum is\", minimum1)\n",
    "minimum2 = fmin(f, 2.0)\n",
    "print(\"Start search at x=2., minimum is\", minimum2)\n",
    "\n",
    "# plot function\n",
    "plt.figure()\n",
    "x = arange(-10, 10, 0.1)\n",
    "y = f(x)\n",
    "plt.plot(x, y, label='$\\cos(x)-3e^{-(x-0.2)^2}$')\n",
    "plt.xlabel('x')\n",
    "plt.grid()\n",
    "plt.axis([-5, 5, -2.2, 0.5])\n",
    "\n",
    "# add minimum1 to plot\n",
    "plt.plot(minimum1, f(minimum1), 'vr',\n",
    "           label='minimum 1')\n",
    "# add start1 to plot\n",
    "plt.plot(1.0, f(1.0), 'or', label='start 1')\n",
    "\n",
    "# add minimum2 to plot\n",
    "plt.plot(minimum2,f(minimum2),'vg',\\\n",
    "           label='minimum 2')\n",
    "# add start2 to plot\n",
    "plt.plot(2.0,f(2.0),'og',label='start 2')\n",
    "\n",
    "plt.legend(loc='lower left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## References\n",
    "\n",
    "\n",
    "- Optimization and Solving Systems of Equations in Julia\n",
    "\n",
    "https://github.com/pkofod/JC2017\n",
    "\n",
    "\n",
    "\n",
    "https://www.youtube.com/watch?v=E_UlaGoObTw\n",
    "\n",
    "\n",
    "Using optimization routines from scipy and statsmodels\n",
    "http://people.duke.edu/~ccc14/sta-663-2017/14C_Optimization_In_Python.html\n",
    "http://people.duke.edu/~ccc14/sta-663-2017/14B_Multivariate_Optimization.html#convexity\n",
    "\n",
    "\n",
    "http://people.duke.edu/~ccc14/sta-663-2017/14B_Multivariate_Optimization.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
