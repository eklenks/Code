{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Nonlinear Equations and Complementarity Problems "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m get_ipython()\u001b[39m.\u001b[39mrun_line_magic(\u001b[39m'\u001b[39m\u001b[39mconfig\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mInlineBackend.figure_format = \u001b[39m\u001b[39m'\u001b[39m\u001b[39mretina\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[39m#https://www.dataquest.io/blog/jupyter-notebook-tips-tricks-shortcuts/\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mseaborn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39msns\u001b[39;00m\n\u001b[1;32m     29\u001b[0m sns\u001b[39m.\u001b[39mset()\n\u001b[1;32m     30\u001b[0m \u001b[39m#sns.set_style(style= \"whitegrid\")\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[39m#plt.style.available\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[39m# bold graph style\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from numpy import append, array, diagonal, tril, triu\n",
    "from numpy.linalg import inv\n",
    "from scipy.linalg import lu\n",
    "#from scipy.linalg import solve\n",
    "from pprint import pprint\n",
    "from numpy import array, zeros, diag, diagflat, dot\n",
    "\n",
    "from sympy import *\n",
    "import sympy as sym\n",
    "init_printing()\n",
    "\n",
    "import matplotlib as mpl\n",
    "# matplotlib for ploting\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D     # 3d\n",
    "# for inline interactive plotting\n",
    "%matplotlib notebook\n",
    "\n",
    "#mpl.rcParams['savefig.dpi'] = 80\n",
    "mpl.rcParams['figure.dpi'] = 80\n",
    "# from IPython.display import set_matplotlib_formats\n",
    "# set_matplotlib_formats('png', 'pdf')\n",
    "# for better picture quality\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "#https://www.dataquest.io/blog/jupyter-notebook-tips-tricks-shortcuts/\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "#sns.set_style(style= \"whitegrid\")\n",
    "#plt.style.available\n",
    "# bold graph style\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Newton's Method\n",
    "\n",
    "\n",
    "In practice, most non-linear root-finding problems are solved using *Newton's method*\n",
    "or one of its variants. Newton's method is based on the principle of *successive linearization*. Successive linearization calls for a hard non-linear problem to be replaced\n",
    "with a sequence of simpler linear problems whose solutions converge to the solution\n",
    "of the non-linear problem. Newton's method is typically formulated as a root-finding\n",
    "technique, but may be used to solve a fixed-point problem $x = g(x)$ by recasting it\n",
    "as the root-finding problem $f(x) = x - g(x) = 0$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$x:f(x)=0\\,.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method starts with a function $f$ defined over the real numbers $x$, the function's derivative $f ′$, and an initial guess $x_0$ for a root of the function $f$. If the function satisfies the assumptions made in the derivation of the formula and the initial guess is close, then a better approximation $x_1$ is\n",
    "\n",
    "$$ x_{1}=x_{0}-{\\frac {f(x_{0})}{f'(x_{0})}}\\,.$$\n",
    "\n",
    "\n",
    "Geometrically, $(x_1, 0)$ is the intersection of the x-axis and the tangent of the graph of f at $(x_0, f (x_0))$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process is repeated as\n",
    "\n",
    "$$ x_{n+1}=x_{n}-{\\frac {f(x_{n})}{f'(x_{n})}}\\,$$\n",
    "until a sufficiently accurate value is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Derivation\n",
    "\n",
    "Suppose $f : [a, b] → ℝ$ is a differentiable function defined on the interval $[a, b]$ with values in the real numbers $ℝ$. The formula for converging on the root can be easily derived. Suppose we have some current approximation $x_{n}$. Then we can derive the formula for a better approximation, $x_{n + 1}$ by referring to the diagram below. The equation of the tangent line to the curve $y = f (x)$ at the point $x = x_n$ is\n",
    "\n",
    "\n",
    "\n",
    "$$ y=f'(x_{n})\\,(x-x_{n})+f(x_{n}),$$\n",
    "\n",
    "\n",
    "where $f′$ denotes the derivative of the function $f$.\n",
    "\n",
    "\n",
    "The x-intercept of this line (the value of $x$ such that $y = 0$) is then used as the next approximation to the root, $x_{n+1}$. In other words, setting $y$ to zero and $x$ to $x_{n+1}$ gives\n",
    "\n",
    "$$ 0=f'(x_{n})\\,(x_{n+1}-x_{n})+f(x_{n}).$$\n",
    "Solving for $x_{n+1}$ gives\n",
    "\n",
    "$$ {\\displaystyle x_{n+1}=x_{n}-{\\frac {f(x_{n})}{f'(x_{n})}}.}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/e/e0/NewtonIteration_Ani.gif/450px-NewtonIteration_Ani.gif)\n",
    "\n",
    "ref: \n",
    "https://en.wikipedia.org/wiki/Newton%27s_method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In SciPy this algorithm is implemented by scipy.optimize.newton\n",
    "\n",
    "Unlike bisection, the Newton-Raphson method uses local slope information\n",
    "\n",
    "This is a double-edged sword:\n",
    "\n",
    "- When the function is well-behaved, the Newton-Raphson method is faster than bisection\n",
    "- When the function is less well-behaved, Newton-Raphson might fail\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/latex": [
       "$\\displaystyle 1.25992104989487$"
      ],
      "text/plain": [
       "1.2599210498948734"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's consider the same example we solved with bisection\n",
    "from scipy.optimize import newton\n",
    "\n",
    "f = lambda x: x**3 -2\n",
    "\n",
    "newton(f, 0.2)   # Start the procedure with the initial guess x = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/latex": [
       "$\\displaystyle 1.18920711500274$"
      ],
      "text/plain": [
       "1.1892071150027401"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's consider another simple example from the slides\n",
    "from scipy.optimize import newton\n",
    "\n",
    "f = lambda x: x**4 -2\n",
    "\n",
    "newton(f, 2.3)   # Start the procedure with the initial guess x = 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final iteration is: 7\n",
      "The computed root is: 1.189207115002721\n"
     ]
    }
   ],
   "source": [
    "# Let's consider another simple example from the slides without using scipy\n",
    "\n",
    "x = 2.3\n",
    "for it in range(100):\n",
    "    deltax = -(x**4.0 - 2.0)/(4.0*x**3.0)\n",
    "    x = x + deltax\n",
    "    if abs(deltax) < 1.e-10: #  accuracy\n",
    "        break\n",
    "        \n",
    "print('The final iteration is:',it)\n",
    "print('The computed root is:',x)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newton \n",
    "\n",
    "Signature: `newton(func, x0, fprime=None, args=(), tol=1.48e-08, maxiter=50, fprime2=None)`\n",
    "\n",
    "Docstring:\n",
    "Find a zero using the Newton-Raphson or secant method.\n",
    "\n",
    "Find a zero of the function `func` given a nearby starting point `x0`.\n",
    "The Newton-Raphson method is used if the derivative `fprime` of `func`\n",
    "is provided, otherwise the secant method is used.  If the second order\n",
    "derivative `fprime2` of `func` is provided, parabolic Halley's method\n",
    "is used.\n",
    "\n",
    "\n",
    "### Parameters\n",
    "\n",
    "\n",
    "func : function\n",
    "\n",
    "    The function whose zero is wanted. It must be a function of a\n",
    "    **single variable** of the form f(x,a,b,c...), where a,b,c... are extra\n",
    "    arguments that can be passed in the `args` parameter.\n",
    "    \n",
    "x0 : float\n",
    "\n",
    "    An initial estimate of the zero that should be somewhere near the\n",
    "    actual zero.\n",
    "    \n",
    "fprime : function, optional\n",
    "\n",
    "    The derivative of the function when available and convenient. If it\n",
    "    is None (default), then the secant method is used.\n",
    "\n",
    "\n",
    "\n",
    "Since `newton` only takes one variable function, for multivariate problems we can use `fsolve` in scipy.optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Cournot Duopoly\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate Newton's method in multiple dimensions, consider a simple Cournot duopoly model, in which the inverse demand for a good is \t$P(q_{1},q_{2})=\\left( \\dfrac{1}{q_{1}+q_{2}} \\right)^{\\epsilon}$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example from the textbook\n",
    "q = np.array([1, 1]) # initial point/guess quantity\n",
    "c = np.array([0.6, 0.8]) # cost vector\n",
    "eta = 1.6 # elasticity \n",
    "e = -1/eta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.68765683, 0.55048639])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cournot example. One update (first step) of newton algorithm \n",
    "fval = np.sum(q)**e + e * np.sum(q)**(e-1)*(q) - np.diag(c).dot(q) # function value\n",
    "fjac = e*np.sum(q)**(e-1)*(np.ones([2,2]))+e*np.sum(q)**(e-1)*(np.eye(2))+\\\n",
    "(e-1)*e*np.sum(q)**(e-2)*(q).dot(np.array([1, 1]))-np.diag(c) # jacobian\n",
    "q = q - np.linalg.inv(fjac).dot(fval) # newton update for the quantity\n",
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.1542114, -0.3542114])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first step result of objective value, and it should go to zero gradually\n",
    "fval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.67598669,  0.12664449],\n",
       "       [ 0.12664449, -0.87598669]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first step result of jacobian\n",
    "fjac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Newton's method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final iteration is: 102\n",
      "The computed root is q: [0.8395676  0.68879643]\n",
      "The function value is f: [5.27553889e-10 6.43030407e-10]\n"
     ]
    }
   ],
   "source": [
    "# Example from the textbook\n",
    "# This executes a finite numer of Newton iterations\n",
    "maxit = 150 # maximum number of iterations\n",
    "tol = 10e-10 # tolerance level for convergence\n",
    "# starting from [0.2 0.2] will cause a convergence problem (Newton's method has limitations)\n",
    "#q = np.array([1, 1])\n",
    "#c = np.array([0.6, 0.8])\n",
    "#eta = 1.6\n",
    "#e = -1/eta\n",
    "\n",
    "for it in np.arange(maxit):\n",
    "\n",
    "    fval = np.sum(q)**e + e * np.sum(q)**(e-1)*(q) - np.diag(c).dot(q)\n",
    "    fjac = e*np.sum(q)**(e-1)*(np.ones([2,2]))+e*np.sum(q)**(e-1)*(np.eye(2))+\\\n",
    "    (e-1)*e*np.sum(q)**(e-2)*(q).dot(np.array([1, 1]))-np.diag(c)\n",
    "    q = q - np.linalg.inv(fjac).dot(fval)\n",
    "    if np.linalg.norm(fval)<tol:\n",
    "        break\n",
    "        \n",
    "print('The final iteration is:',it)\n",
    "print('The computed root is q:',q)\n",
    "print('The function value is f:',fval) # it should be close to zero when a root is found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's write a function (mynewton) that achieves the same goal: solving a simple cournot model. It calls another function (cournot) that for the current guess computes the value of the FOC's for the two firms, and the related Jacobian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cournot(q):\n",
    "    \"\"\"\n",
    "    A function for the Cournot game\n",
    "    input: q quantity of production\n",
    "    output: \n",
    "        fval: objective/function value,could be vector\n",
    "        fjac: value of Jacobian of function, could be matrix\n",
    "    \"\"\"\n",
    "    c = np.array([0.6, 0.8])\n",
    "    eta = 1.6\n",
    "    e = -1 / eta\n",
    "    fval = np.sum(q)**e + e * np.sum(q)**(e-1)*(q) - np.diag(c).dot(q)\n",
    "    fjac = e*np.sum(q)**(e-1)*(np.ones([2,2]))+e*np.sum(q)**(e-1)*(np.eye(2))+\\\n",
    "            (e-1)*e*np.sum(q)**(e-2)*(q).dot(np.array([1, 1]))-np.diag(c)\n",
    "    return fval, fjac\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Newton \n",
    "def mynewton(f, x0, maxit=1000, tol=10e-10 ):\n",
    "    \"\"\"\n",
    "    input: \n",
    "        f: function which output fval and fjac\n",
    "        x0: initial guess of solution\n",
    "        maxit: maximum number of iteration\n",
    "        tol： tolerance\n",
    "    output:\n",
    "        x： solution        \n",
    "    \"\"\"\n",
    "    x = x0\n",
    "    for it in np.arange(maxit):\n",
    "        fval, fjac = f(x)\n",
    "        x = x - np.linalg.inv(fjac).dot(fval)\n",
    "        if np.linalg.norm(fval)<tol:\n",
    "            break\n",
    "    return x    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.8395676 , 0.68879643])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mynewton(cournot, x0 = np.array([1, 1])) # it does converge to the same solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if we use a poor initial guess, the method might not work.\n",
    "Without backstepping, an appropriate correction of the updates, the iterations do not converge with x0= np.array([0.2, 0.2]). The issue is that some updates are overstepping too far, entering an unstable region.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_162/970703703.py:12: RuntimeWarning: invalid value encountered in scalar power\n",
      "  fval = np.sum(q)**e + e * np.sum(q)**(e-1)*(q) - np.diag(c).dot(q)\n",
      "/tmp/ipykernel_162/970703703.py:13: RuntimeWarning: invalid value encountered in scalar power\n",
      "  fjac = e*np.sum(q)**(e-1)*(np.ones([2,2]))+e*np.sum(q)**(e-1)*(np.eye(2))+\\\n",
      "/tmp/ipykernel_162/970703703.py:14: RuntimeWarning: invalid value encountered in scalar power\n",
      "  (e-1)*e*np.sum(q)**(e-2)*(q).dot(np.array([1, 1]))-np.diag(c)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([nan, nan])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mynewton(cournot, x0 = np.array([0.2, 0.2])) # the procedure breaks down, and Python prompts an error message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more robust implementation can be found here\n",
    "https://github.com/randall-romero/CompEcon-python/blob/master/compecon/nonlinear.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Libraries: the fsolve function\n",
    "\n",
    "Python has a number of libraries designed to deal with root finding problems. Since scipy.optimize.newton only takes functions with one variable, for this application we can use fsolve in scipy.optimize to find the roots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The root x is approximately x= 2.000000000000006661\n",
      "The error is -6.66134e-15.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_162/2743209447.py:9: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  print(\"The root x is approximately x=%21.19g\" % x)\n",
      "/tmp/ipykernel_162/2743209447.py:10: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  print(\"The error is %g.\" % (2 - x))\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import fsolve\n",
    "\n",
    "## Let's first check the library with a simple example \n",
    "def f(x):\n",
    "    return x ** 3 - 2 * x ** 2\n",
    "\n",
    "x = fsolve(f, 3)           # one root is at x=2.0\n",
    "\n",
    "print(\"The root x is approximately x=%21.19g\" % x)\n",
    "print(\"The error is %g.\" % (2 - x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "\n",
    "The return value of `fsolve` is a numpy array of length n for a root finding problem with `n` variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's consider again the Cournot example\n",
    "def cournotfval(q):\n",
    "    \"\"\"only output objective/function value\"\"\"\n",
    "    c = np.array([0.6, 0.8])\n",
    "    eta = 1.6\n",
    "    e = -1/eta\n",
    "    fval = np.sum(q)**e + e * np.sum(q)**(e-1)*(q) - np.diag(c).dot(q)\n",
    "    return fval\n",
    "\n",
    "def cournotjac(q):\n",
    "    \"\"\"only output value of jacobian of function \"\"\"\n",
    "    c = np.array([0.6, 0.8])\n",
    "    eta = 1.6\n",
    "    e = -1/eta\n",
    "    fjac = e*np.sum(q)**(e-1)*(np.ones([2,2]))+e*np.sum(q)**(e-1)*(np.eye(2))+(e-1)*e*np.sum(q)**(e-2)*(q).dot(np.array([1, 1]))-np.diag(c)\n",
    "    return fjac    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The computed root is q: [0.8395676  0.68879643]\n"
     ]
    }
   ],
   "source": [
    "# Here the library function fsolve is using the analytic jacobian\n",
    "# The procedure works even with the poor inital guess\n",
    "q = fsolve(func = cournotfval,x0= np.array([0.2, 0.2]), fprime= cournotjac)  # with analytic jacobian, computed by cournotjac\n",
    "print('The computed root is q:',q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The computed root is q: [0.8395676  0.68879643]\n"
     ]
    }
   ],
   "source": [
    "# Here the library function fsolve is not using the analytic jacobian (it's replaced with a numerical approximation)\n",
    "# The procedure works even with the poor inital guess\n",
    "#q = fsolve(func = cournotfval, x0= np.array([1.0, 1.0]))\n",
    "q = fsolve(func = cournotfval, x0= np.array([0.2, 0.2]))\n",
    "print('The computed root is q:',q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "3d597f4c481aa0f25dceb95d2a0067e73c0966dcbd003d741d821a7208527ecf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
